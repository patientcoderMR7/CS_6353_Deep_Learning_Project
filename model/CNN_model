#!/usr/bin/env python
# coding: utf-8

# In[5]:


import os
import tensorflow 
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt 
import numpy as np
from keras.utils.np_utils import to_categorical
import random,shutil
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout,Conv2D,Flatten,Dense, MaxPooling2D, BatchNormalization, Activation
from tensorflow.keras.models import load_model


def generator(dir, gen=image.ImageDataGenerator(rescale=1./255), shuffle=True,batch_size=1,target_size=(24,24),class_mode='categorical' ):

    return gen.flow_from_directory(dir,batch_size=batch_size,shuffle=shuffle,color_mode='grayscale',class_mode=class_mode,target_size=target_size)

BS= 32
TS=(24,24)
train_batch= generator('CEW dataset_B_Eye_Images\custom\data train',shuffle=True, batch_size=BS,target_size=TS)
valid_batch= generator('\CEW dataset_B_Eye_Images\custom\data test',shuffle=True, batch_size=BS,target_size=TS)
SPE= len(train_batch.classes)//BS
VS = len(valid_batch.classes)//BS
#print(SPE,VS)


img,labels= next(train_batch)
# print(img.shape)

model = Sequential([
    Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(24,24,1)),
    #MaxPooling2D(pool_size=(1,1)),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D(pool_size=(1,1)),
#32 convolution filters used each of size 3x3
#again
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(1,1)),

#64 convolution filters used each of size 3x3
#choose the best features via pooling
    
#randomly turn neurons on and off to improve convergence
    Dropout(0.25),
#flatten since too many dimensions, we only want a classification output
    Flatten(),
#fully connected to get all relevant data
    Dense(128, activation='relu'),
#one more dropout for convergence' sake :) 
    Dropout(0.5),
#output a softmax to squash the matrix into output probabilities
    Dense(2, activation='sigmoid')
    #model.add(Activation(tensorflow.nn.softmax))
])
print(model.summary())

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model.fit(train_batch, validation_data=valid_batch,epochs=2,steps_per_epoch=SPE ,validation_steps=VS)

#model.save('D:\MR\Academics\B.E Project\Drowsiness detection\models/newmodel1.h5', overwrite=True)


# In[6]:


for layers in model.layers:
  try:
    filters, biases=layers.get_weights()
  #print(weights)
    print(layers.name, filters.shape)
  except:
    pass  


# In[ ]:




